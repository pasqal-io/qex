{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests with Charles for Horqrux\n",
    "I tested here also if the too long compilation was solved here in JAX\n",
    "# Noise models in Global QNN variants.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAX_CHECK_TRACER_LEAKS\"] = \"True\"\n",
    "# set env var JAX_CHECK_TRACER_LEAKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.35\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "AttributeError: module 'qedft' has no attribute '__version__'\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import jax; import qedft; print(jax.__version__); print(qedft.__version__)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/igorsokolov/PycharmProjects/qex/.venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from qedft.models.networks import LocalQNN, GlobalQNN, GlobalQNNwithMLP, GlobalQiCQNN, GlobalQiQNN\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Density shape: (9,)\n",
      "Grids shape: (9,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-24 15:40:37.403\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mqedft.models.quantum.quantum_models\u001b[0m:\u001b[36mbuild_qnn\u001b[0m:\u001b[36m683\u001b[0m - \u001b[1mBuilding LocalQNN QNN with DirectQNN layer_type\u001b[0m\n",
      "\u001b[32m2025-07-24 15:40:40.547\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mqedft.models.quantum.quantum_models\u001b[0m:\u001b[36mbuild_qnn\u001b[0m:\u001b[36m678\u001b[0m - \u001b[1mProductQNN can generate NaNs due to domain restriction of the arcsin function. Modify the map_fn (default: jnp.arcsin) to avoid this issue or normalize the input data.\u001b[0m\n",
      "\u001b[32m2025-07-24 15:40:40.547\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mqedft.models.quantum.quantum_models\u001b[0m:\u001b[36mbuild_qnn\u001b[0m:\u001b[36m683\u001b[0m - \u001b[1mBuilding LocalQNN QNN with ProductQNN layer_type\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DirectQNN ===\n",
      "Output shape: (9,)\n",
      "Output values: [1.95396874 1.93403894 1.86914238 1.76210312 1.61768188 1.44230961\n",
      " 1.24372743 1.03055464 0.81181101]\n",
      "\n",
      "=== ProductQNN ===\n",
      "Output shape: (9,)\n",
      "Output values: [1.97996765 1.9737837  1.92034164 1.81788923 1.66336047 1.451347\n",
      " 1.17124902 0.79621778 0.05810413]\n"
     ]
    }
   ],
   "source": [
    "# Create different QNN models\n",
    "models = {\n",
    "    \"DirectQNN\": LocalQNN(config_dict={\"qnn_type\": \"LocalQNN\", \"layer_type\": \"DirectQNN\"}),\n",
    "    \"ProductQNN\": LocalQNN(config_dict={\"qnn_type\": \"LocalQNN\", \"layer_type\": \"ProductQNN\"}),\n",
    "}\n",
    "\n",
    "# Setup test data\n",
    "num_points = 9\n",
    "density = jnp.linspace(0, 1, num_points)\n",
    "print(f\"Density shape: {density.shape}\")\n",
    "grids = jnp.ones(density.shape)\n",
    "print(f\"Grids shape: {grids.shape}\")\n",
    "rng_key = jax.random.PRNGKey(0)\n",
    "\n",
    "# Test each model\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    # Build network\n",
    "    init_fn, apply_fn = model.build_network(grids)\n",
    "\n",
    "    # Initialize parameters and run inference\n",
    "    _, params = init_fn(rng_key, input_shape=(-1, num_points, 1))\n",
    "    output = apply_fn(params, density)\n",
    "\n",
    "    # Store results\n",
    "    results[name] = output\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Output values: {output}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Data for the best and worst parameters across all devices:\n",
    "\n",
    "=== Individual Backend Analysis ===\n",
    "\n",
    "--- ibm_brisbane with 127 qubits ---\n",
    "=== Quantum Device Parameters ===\n",
    "Max T1 time: 433.44 ns\n",
    "Max T2 time: 403.38 ns\n",
    "Max single-qubit gate time: 60.00 ns\n",
    "Max two-qubit gate time: 780.00 ns\n",
    "Max reset gate time: 1860.00 ns\n",
    "Max readout error: 0.274170\n",
    "Average T1 time: 233.84 ns\n",
    "Average T2 time: 161.31 ns\n",
    "\n",
    "=== Error Probabilities ===\n",
    "Gate time used: 60.00 ns\n",
    "Amplitude damping probability: 0.129274\n",
    "Phase damping probability: 0.076450\n",
    "Readout error probability: 0.274170\n",
    "--------------------------------\n",
    "\n",
    "--- ibm_sherbrooke with 127 qubits ---\n",
    "=== Quantum Device Parameters ===\n",
    "Max T1 time: 515.54 ns\n",
    "Max T2 time: 492.86 ns\n",
    "Max single-qubit gate time: 56.89 ns\n",
    "Max two-qubit gate time: 881.78 ns\n",
    "Max reset gate time: 1400.89 ns\n",
    "Max readout error: 0.212158\n",
    "Average T1 time: 282.26 ns\n",
    "Average T2 time: 186.66 ns\n",
    "\n",
    "=== Error Probabilities ===\n",
    "Gate time used: 56.89 ns\n",
    "Amplitude damping probability: 0.104477\n",
    "Phase damping probability: 0.058474\n",
    "Readout error probability: 0.212158\n",
    "--------------------------------\n",
    "\n",
    "--- ibm_kyiv with 127 qubits ---\n",
    "=== Quantum Device Parameters ===\n",
    "Max T1 time: 593.86 ns\n",
    "Max T2 time: 533.33 ns\n",
    "Max single-qubit gate time: 49.78 ns\n",
    "Max two-qubit gate time: 583.11 ns\n",
    "Max reset gate time: 1294.22 ns\n",
    "Max readout error: 0.504395\n",
    "Average T1 time: 279.10 ns\n",
    "Average T2 time: 142.49 ns\n",
    "\n",
    "=== Error Probabilities ===\n",
    "Gate time used: 49.78 ns\n",
    "Amplitude damping probability: 0.080404\n",
    "Phase damping probability: 0.050124\n",
    "Readout error probability: 0.504395\n",
    "--------------------------------\n",
    "=== Best Parameters Across All Devices ===\n",
    "Largest T1 time: 593.86 ns (from ibm_kyiv)\n",
    "Largest T2 time: 533.33 ns (from ibm_kyiv)\n",
    "Fastest single-qubit gate: 49.78 ns (from ibm_kyiv)\n",
    "Fastest two-qubit gate: 583.11 ns (from ibm_kyiv)\n",
    "Lowest readout error: 0.212158 (from ibm_sherbrooke)\n",
    "\n",
    "=== Worst Parameters Across All Devices ===\n",
    "Smallest T1 time: 433.44 ns (from ibm_brisbane)\n",
    "Smallest T2 time: 403.38 ns (from ibm_brisbane)\n",
    "Slowest single-qubit gate: 60.00 ns (from ibm_brisbane)\n",
    "Slowest two-qubit gate: 881.78 ns (from ibm_sherbrooke)\n",
    "Highest readout error: 0.504395 (from ibm_kyiv)\n",
    "\n",
    "=== Error Probabilities with Best Parameters ===\n",
    "\n",
    "=== Error Probabilities ===\n",
    "Gate time used: 49.78 ns\n",
    "Amplitude damping probability: 0.080404\n",
    "Phase damping probability: 0.050124\n",
    "Readout error probability: 0.212158\n",
    "\n",
    "=== Error Probabilities with Worst Parameters ===\n",
    "\n",
    "=== Error Probabilities ===\n",
    "Gate time used: 60.00 ns\n",
    "Amplitude damping probability: 0.129274\n",
    "Phase damping probability: 0.076450\n",
    "Readout error probability: 0.504395\n",
    "\n",
    "\n",
    "Error rates I can use:\n",
    "\n",
    "| Error Source                 | Noise Channel        | Value (Best Case)         | Value (Worst Case)      | Source                    |\n",
    "| ---------------------------- | -------------------- | ------------------------- | ----------------------- | ------------------------- |\n",
    "| **Readout error (SPAM)**     | `BITFLIP`            | 0.212158                  | 0.504395                | `readout error` prob      |\n",
    "| **Preparation error (SPAM)** | `BITFLIP`            | 0.212158 *(assumed same)* | 0.504395 *(same)*       | Approximation             |\n",
    "| **Energy relaxation (T₁)**   | `AMPLITUDE_DAMPING`  | 0.080404                  | 0.129274                | From gate-time + T₁       |\n",
    "| **Dephasing (T₂)**           | `PHASE_DAMPING`      | 0.050124                  | 0.076450                | From gate-time + T₂       |\n",
    "| **Gate infidelity**          | `DEPOLARIZING`       | *Not directly provided*   | *Not directly provided* | Estimate from calibration |\n",
    "| **Coherent crosstalk / ZZ**  | *Custom error model* | *Not available*           | *Not available*         | Not in current data       |\n",
    "\n",
    "\n",
    "If you're simulating a realistic IBM QPU based on your data:\n",
    "\n",
    "Use:\n",
    "BITFLIP(p=0.21–0.50) → SPAM\n",
    "AMPLITUDE_DAMPING(p=0.08–0.13) → T₁\n",
    "PHASE_DAMPING(p=0.05–0.076) → T₂\n",
    "DEPOLARIZING(p=0.001–0.01) → estimated gate infidelity\n",
    "\n",
    "For Pasqal QPUs, we'd drop depolarizing and use:\n",
    "BITFLIP for SPAM\n",
    "PHASE_DAMPING + AMPLITUDE_DAMPING (estimated from pulse decoherence)\n",
    "Possibly model laser detuning/stochastic phase with custom Lindblad ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model \n",
    "We need the same Local QNN as in the convolutional in Qiskit I used.\n",
    "\n",
    "For QNN in Qiskit I used this. \n",
    "\n",
    "This is a global model. It is the same in the paper. \n",
    "We can use the same as in the paper also for the noise study.\n",
    "```python\n",
    "def create_stax_model(n_qubits=2, n_layers=1, hidden_size=4, output_size=1):\n",
    "    return stax.serial(\n",
    "        QiskitQNNLayer(n_qubits, n_layers),  # quantum layer (trainable via param shift)\n",
    "        stax.Dense(hidden_size),\n",
    "        stax.Relu,\n",
    "        stax.Dense(output_size),\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qedft.models.quantum.convolutional_models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  Global QNN\n",
    "\n",
    "# # Test configuration\n",
    "# input_dimension = 513\n",
    "# n_qubits = 6\n",
    "# largest_kernel_dimension = n_qubits\n",
    "# n_var_layers = 8\n",
    "# n_out = 1\n",
    "# max_number_conv_layers = 1\n",
    "\n",
    "# # Get kernel dimensions and outputs per layer\n",
    "# list_kernel_dimensions, list_outputs_per_conv_layer = compute_kernel_width_per_layer(\n",
    "#     input_dimension=input_dimension,\n",
    "#     largest_kernel_dimension=largest_kernel_dimension,\n",
    "#     max_number_conv_layers=max_number_conv_layers,\n",
    "# )\n",
    "# print(\"Kernel dimensions:\", list_kernel_dimensions)\n",
    "# print(\"Outputs per layer:\", list_outputs_per_conv_layer)\n",
    "\n",
    "# # Construct model\n",
    "# list_conv_layers = construct_convolutional_model(\n",
    "#     n_qubits=n_qubits,\n",
    "#     n_var_layers=n_var_layers,\n",
    "#     n_out=n_out,\n",
    "#     input_dimension=input_dimension,\n",
    "#     largest_kernel_width=largest_kernel_dimension,\n",
    "#     max_number_conv_layers=max_number_conv_layers,\n",
    "#     use_qnn_layers=True,  #\n",
    "#     use_bias_mlp=True,\n",
    "#     use_amplitude_encoding=False,\n",
    "# )\n",
    "\n",
    "# # Create test input\n",
    "# grids = jnp.ones(input_dimension)\n",
    "\n",
    "# # Test each layer individually\n",
    "# current_input = grids\n",
    "# for i, (init_fn, apply_fn) in enumerate(list_conv_layers):\n",
    "#     _, params = init_fn(jax.random.PRNGKey(i), (input_dimension,))\n",
    "#     # print(params)\n",
    "#     current_input = apply_fn(params, current_input)\n",
    "#     print(f\"Layer {i} output shape:\", current_input.shape)\n",
    "\n",
    "# # Test full model using stax\n",
    "# network = stax.serial(*list_conv_layers)\n",
    "# init_fn, apply_fn = network\n",
    "# _, params = init_fn(jax.random.PRNGKey(0), (input_dimension,))\n",
    "\n",
    "# # print(params)\n",
    "# print(len(params[0]))\n",
    "# print(len(params[1][0]))\n",
    "\n",
    "\n",
    "# # Count parameters\n",
    "# param_count = count_parameters(params)\n",
    "# print(\"Number of parameters:\", param_count)\n",
    "# print(\"We want 316 parameters as in Global QNN\")\n",
    "\n",
    "# # Test forward pass\n",
    "# output = apply_fn(params, grids)\n",
    "# print(\"\\nFull model output shape:\", output.shape)\n",
    "# print(\"Output:\", output)\n",
    "\n",
    "# # Test with JIT compilation\n",
    "# jit_apply_fn = jax.jit(apply_fn)\n",
    "# jit_output = jit_apply_fn(params, grids)\n",
    "# print(\"\\nJIT output matches:\", jnp.allclose(output, jit_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jit_output = jit_apply_fn(params, grids)\n",
    "# jit_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise\n",
    "from typing import Callable\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import pytest\n",
    "\n",
    "from horqrux.api import expectation, run, sample\n",
    "from horqrux.apply import apply_gates\n",
    "from horqrux.noise import DigitalNoiseInstance, DigitalNoiseType\n",
    "from horqrux.primitives.parametric import PHASE, RX, RY, RZ\n",
    "from horqrux.primitives.primitive import NOT, H, I, S, T, X, Y, Z\n",
    "from horqrux.utils.operator_utils import density_mat, product_state, random_state\n",
    "\n",
    "MAX_QUBITS = 7\n",
    "PARAMETRIC_GATES = (RX, RY, RZ, PHASE)\n",
    "PRIMITIVE_GATES = (NOT, H, X, Y, Z, I, S, T)\n",
    "\n",
    "NOISE_single_prob = (\n",
    "    DigitalNoiseType.BITFLIP,\n",
    "    DigitalNoiseType.PHASEFLIP,\n",
    "    DigitalNoiseType.DEPOLARIZING,\n",
    "    DigitalNoiseType.AMPLITUDE_DAMPING,\n",
    "    DigitalNoiseType.PHASE_DAMPING,\n",
    ")\n",
    "ALL_NOISES = list(DigitalNoiseType)\n",
    "\n",
    "\n",
    "def noise_instance(noise_type: DigitalNoiseType) -> DigitalNoiseInstance:\n",
    "    if noise_type in NOISE_single_prob:\n",
    "        errors = 0.1\n",
    "    elif noise_type == DigitalNoiseType.PAULI_CHANNEL:\n",
    "        errors = (0.4, 0.5, 0.1)\n",
    "    else:\n",
    "        errors = (0.2, 0.8)\n",
    "\n",
    "    return DigitalNoiseInstance(noise_type, error_probability=errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise = (DigitalNoiseInstance(DigitalNoiseType.DEPOLARIZING, 0.1),)\n",
    "# ops = [X(0, noise=noise), X(1)]\n",
    "# state = product_state(\"00\")\n",
    "# # state_output = run(ops, state)\n",
    "# # test sampling\n",
    "# dm_state = density_mat(state)\n",
    "# sampling_output = sample(\n",
    "#     dm_state,\n",
    "#     ops,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from horqrux import QuantumCircuit, X, apply_gates, expectation, random_state, zero_state\n",
    "from qedft.models.quantum.measurement import (\n",
    "    qubit_magnetization,\n",
    "    total_magnetization,\n",
    "    total_magnetization_ops,\n",
    "    total_magnetization_via_inner_product,\n",
    ")\n",
    "# Create noise instances\n",
    "low_noise = (DigitalNoiseInstance(DigitalNoiseType.BITFLIP, 0.0), )\n",
    "high_noise = (DigitalNoiseInstance(DigitalNoiseType.BITFLIP, 0.01),)\n",
    "\n",
    "\n",
    "# Create circuits with different noise levels\n",
    "gates_no_noise = [RX(\"x\", 0), RX(\"y\", 1)]\n",
    "gates_low_noise = [RX(\"x\", 0, noise=low_noise), RX(\"y\", 1, noise=low_noise)]\n",
    "gates_high_noise = [RX(\"x\", 0, noise=high_noise), RX(\"y\", 1, noise=high_noise)]\n",
    "\n",
    "circuit_no_noise = QuantumCircuit(4, gates_no_noise)\n",
    "circuit_low_noise = QuantumCircuit(4, gates_low_noise)\n",
    "circuit_high_noise = QuantumCircuit(4, gates_high_noise)\n",
    "\n",
    "# Create measurement operators\n",
    "z_ops = total_magnetization_ops(4)\n",
    "\n",
    "# Parameters for parametric gates\n",
    "values = {\"x\": jnp.array([0.0]), \"y\": jnp.array([0.0])}\n",
    "\n",
    "# Test with zero state\n",
    "state = zero_state(4)\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "mag_no_noise = expectation(state, circuit_no_noise, z_ops, values, key=key)\n",
    "mag_low_noise = expectation(state, circuit_low_noise, z_ops, values, key=key)\n",
    "mag_high_noise = expectation(state, circuit_high_noise, z_ops, values, key=key)\n",
    "\n",
    "# Zero noise should match no noise\n",
    "np.testing.assert_allclose(jnp.sum(mag_no_noise), jnp.sum(mag_low_noise), rtol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1.]\n",
      "[1. 1. 1. 1.]\n",
      "[0.98 0.98 1.   1.  ]\n"
     ]
    }
   ],
   "source": [
    "print(mag_no_noise)\n",
    "print(mag_low_noise)\n",
    "print(mag_high_noise)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "| Error Source                 | Noise Channel        | Value (Best Case)         | Value (Worst Case)      | Source                    |\n",
    "| ---------------------------- | -------------------- | ------------------------- | ----------------------- | ------------------------- |\n",
    "| **Readout error (SPAM)**     | `BITFLIP`            | 0.212158                  | 0.504395                | `readout error` prob      |\n",
    "| **Preparation error (SPAM)** | `BITFLIP`            | 0.212158 *(assumed same)* | 0.504395 *(same)*       | Approximation             |\n",
    "| **Energy relaxation (T₁)**   | `AMPLITUDE_DAMPING`  | 0.080404                  | 0.129274                | From gate-time + T₁       |\n",
    "| **Dephasing (T₂)**           | `PHASE_DAMPING`      | 0.050124                  | 0.076450                | From gate-time + T₂       |\n",
    "| **Gate infidelity**          | `DEPOLARIZING`       | *Not directly provided*   | *Not directly provided* | Estimate from calibration |\n",
    "| **Coherent crosstalk / ZZ**  | *Custom error model* | *Not available*           | *Not available*         | Not in current data       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bitflip rate: 2e-07\n",
      "Amplitude damping rate: 8e-08\n",
      "Phase damping rate: 5e-08\n",
      "[Observable(operations=[Z(target=((0,),), control=((None,),))]), Observable(operations=[Z(target=((1,),), control=((None,),))])]\n",
      "[0.9999996 0.9999996]\n"
     ]
    }
   ],
   "source": [
    "n_qubits = 2\n",
    "\n",
    "bitflip_rate = 0.2 # 0.2\n",
    "amplitude_damping_rate = 0.08 # 0.08\n",
    "phase_damping_rate = 0.05 # 0.05\n",
    "\n",
    "noise_scaling = 1e-6\n",
    "\n",
    "bitflip_rate *= noise_scaling\n",
    "amplitude_damping_rate *= noise_scaling\n",
    "phase_damping_rate *= noise_scaling\n",
    "\n",
    "# Print the noise rates\n",
    "print(f\"Bitflip rate: {bitflip_rate}\")\n",
    "print(f\"Amplitude damping rate: {amplitude_damping_rate}\")\n",
    "print(f\"Phase damping rate: {phase_damping_rate}\")\n",
    "\n",
    "real_noise = (DigitalNoiseInstance(DigitalNoiseType.BITFLIP, bitflip_rate), DigitalNoiseInstance(DigitalNoiseType.AMPLITUDE_DAMPING, amplitude_damping_rate), DigitalNoiseInstance(DigitalNoiseType.PHASE_DAMPING, phase_damping_rate),)\n",
    "\n",
    "# real_noise = (DigitalNoiseInstance(DigitalNoiseType.BITFLIP, 0.5), )\n",
    "\n",
    "\n",
    "# Create measurement operators\n",
    "z_ops = total_magnetization_ops(n_qubits)\n",
    "print(z_ops)\n",
    "\n",
    "# Parameters for parametric gates\n",
    "values = {\"x\": jnp.array([0.0]), \"y\": jnp.array([0.0])}\n",
    "\n",
    "# Test with zero state\n",
    "state = zero_state(n_qubits)\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "gates_real_noise = [RX(\"x\", 0, noise=real_noise), RX(\"y\", 1, noise=real_noise)]\n",
    "circuit_real_noise = QuantumCircuit(n_qubits, gates_real_noise)\n",
    "mag_real_noise = expectation(state, circuit_real_noise, z_ops, values, key=key)\n",
    "\n",
    "print(mag_real_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working example of the Global QNN with noise, DO NOT USE SHOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-24 15:41:01.827\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mqedft.models.quantum.convolutional_models\u001b[0m:\u001b[36mconstruct_convolutional_model\u001b[0m:\u001b[36m180\u001b[0m - \u001b[1mUsing GlobalQNNLayer layer\u001b[0m\n",
      "\u001b[32m2025-07-24 15:41:01.845\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mqedft.models.quantum.convolutional_models\u001b[0m:\u001b[36mconstruct_convolutional_model\u001b[0m:\u001b[36m210\u001b[0m - \u001b[1mGlobalQNNLayer Layer 0: n_qubits_layer 6\u001b[0m\n",
      "\u001b[32m2025-07-24 15:41:02.020\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mqedft.models.quantum.quantum_models\u001b[0m:\u001b[36mbuild_qnn\u001b[0m:\u001b[36m683\u001b[0m - \u001b[1mBuilding GlobalQNN QNN with DirectQNN layer_type\u001b[0m\n",
      "\u001b[32m2025-07-24 15:41:02.020\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mqedft.models.quantum.convolutional_models\u001b[0m:\u001b[36mconstruct_convolutional_model\u001b[0m:\u001b[36m268\u001b[0m - \u001b[1mAdding a single dense layer at the end (outputs last layer 171)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list_kernel_dimensions , list_outputs_per_conv_layer:  [3] [171]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-24 15:41:52.963\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mqedft.train.od.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1mInitialized trainer with config: {'name': 'test', 'experiment_name': 'test', 'network_type': 'conv_dqc', 'molecule_name': 'h2', 'molecule_names': ['h2'], 'dataset1': [128, 384], 'rng': 0, 'save_plot_loss': False, 'save_every_n': 20, 'activation': 'tanh', 'n_neurons': 513, 'n_layers': 2, 'n_qubits': 9, 'n_reupload_layers': 1, 'use_rzz_parametrized_entanglers': False, 'chebychev_reuploading': False, 'add_reversed_rzz': False, 'entangling_block_type': 'alternate_linear', 'single_qubit_rotations': ['rz', 'rx', 'rz'], 'use_same_parameters': False, 'add_negative_transform': False, 'wrap_with_self_interaction_layer': False, 'wrap_with_global_functional': False, 'use_correlators_in_output': False, 'output_operators': ['Z'], 'use_bias_in_output': False, 'max_train_steps': 10000, 'factr': 1.0, 'pgtol': 1e-14, 'm': 20, 'maxfun': 20, 'maxiter': 2, 'num_iterations': 15, 'ks_iter_to_ignore': 10, 'discount_factor': 0.9, 'alpha': 0.5, 'alpha_decay': 0.9, 'num_mixing_iterations': 1, 'density_mse_converge_tolerance': -1.0, 'stop_gradient_step': -1, 'enforce_reflection_symmetry': False, 'use_relative_encoding': False, 'num_grids': 513, 'use_amplitude_encoding': True, 'max_number_conv_layers': 100, 'list_qubits_per_layer': [], 'force_qubits_per_layer_is_kernel_width': False, 'feature_map_type': 'direct', 'final_mlp_layers': [1], 'dont_use_parametrized_observables': True, 'use_nel_as_input_with_mlp': False, 'nel_exc_combination_type': 'sum', 'nel_mlp_layers': [1], 'density_normalization_factor': 2.0, 'use_pmap_for_gpu_per_molecule': False, 'num_global_filters': 16, 'num_local_filters': 16, 'num_local_conv_layers': 2, 'ksr_activation': 'swish', 'ksr_minval': 0.1, 'ksr_maxval': 2.385345, 'ksr_downsample_factor': 0, 'use_exponential_coulomb': True, 'jit_loss': True, 'jit_neural_xc': True}\u001b[0m\n",
      "\u001b[32m2025-07-24 15:41:52.964\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mqedft.data_io.dataset_loader\u001b[0m:\u001b[36mload_molecular_datasets\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mLoading dataset for h2\u001b[0m\n",
      "\u001b[32m2025-07-24 15:41:52.964\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mqedft.data_io.dataset_loader\u001b[0m:\u001b[36mload_molecular_datasets\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mLoading dataset from /Users/igorsokolov/PycharmProjects/qex/data/od/h2\u001b[0m\n",
      "\u001b[32m2025-07-24 15:41:52.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mqedft.data_io.dataset_loader\u001b[0m:\u001b[36mload_molecular_datasets\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1mTraining distances: [128, 384]\u001b[0m\n",
      "\u001b[32m2025-07-24 15:41:52.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mqedft.data_io.dataset_loader\u001b[0m:\u001b[36mload_molecular_datasets\u001b[0m:\u001b[36m101\u001b[0m - \u001b[1mNumber of electrons: 2\u001b[0m\n",
      "\u001b[32m2025-07-24 15:41:52.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mqedft.data_io.dataset_loader\u001b[0m:\u001b[36mload_molecular_datasets\u001b[0m:\u001b[36m102\u001b[0m - \u001b[1mGrid shape: (513,)\u001b[0m\n",
      "\u001b[32m2025-07-24 15:41:53.086\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mqedft.models.quantum.convolutional_models\u001b[0m:\u001b[36mconstruct_convolutional_model\u001b[0m:\u001b[36m180\u001b[0m - \u001b[1mUsing GlobalQNNLayer layer\u001b[0m\n",
      "\u001b[32m2025-07-24 15:41:53.103\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mqedft.models.quantum.convolutional_models\u001b[0m:\u001b[36mconstruct_convolutional_model\u001b[0m:\u001b[36m210\u001b[0m - \u001b[1mGlobalQNNLayer Layer 0: n_qubits_layer 6\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.12796035]\n",
      "list_kernel_dimensions , list_outputs_per_conv_layer:  [3] [171]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-24 15:41:53.236\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mqedft.models.quantum.quantum_models\u001b[0m:\u001b[36mbuild_qnn\u001b[0m:\u001b[36m683\u001b[0m - \u001b[1mBuilding GlobalQNN QNN with DirectQNN layer_type\u001b[0m\n",
      "\u001b[32m2025-07-24 15:41:53.236\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mqedft.models.quantum.convolutional_models\u001b[0m:\u001b[36mconstruct_convolutional_model\u001b[0m:\u001b[36m268\u001b[0m - \u001b[1mAdding a single dense layer at the end (outputs last layer 171)\u001b[0m\n",
      "\u001b[32m2025-07-24 15:41:53.237\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mqedft.models.wrappers\u001b[0m:\u001b[36mwrap_network\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mGlobal model, ensuring output is scalar (wrap_self_interaction causes global models to output (num_grids,) instead of (1,))\u001b[0m\n",
      "\u001b[32m2025-07-24 15:41:53.237\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mqedft.train.od.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mInitializing fresh parameters\u001b[0m\n",
      "\u001b[32m2025-07-24 15:41:53.238\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mqedft.train.od.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m114\u001b[0m - \u001b[1mJitting neural_xc_energy_density_fn\u001b[0m\n",
      "\u001b[32m2025-07-24 15:41:53.283\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mqedft.train.od.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mJitting loss_fn\u001b[0m\n",
      "\u001b[32m2025-07-24 15:41:53.284\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mqedft.train.od.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1mCheckpoint save directory: /Users/igorsokolov/PycharmProjects/qex/tests/ckpts\u001b[0m\n",
      "\u001b[32m2025-07-24 15:41:53.284\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mqedft.train.od.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m173\u001b[0m - \u001b[1mOptimizing with L-BFGS-B: {'maxfun': 20, 'factr': 1.0, 'm': 20, 'pgtol': 1e-14, 'maxiter': 2}\u001b[0m\n",
      "\u001b[32m2025-07-24 15:41:53.287\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mqedft.train.od.train\u001b[0m:\u001b[36m_kohn_sham\u001b[0m:\u001b[36m116\u001b[0m - \u001b[1mJitting kohn_sham_func\u001b[0m\n",
      "\u001b[32m2025-07-24 15:44:35.406\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mqedft.train.od.train\u001b[0m:\u001b[36mnp_value_and_grad_fn\u001b[0m:\u001b[36m454\u001b[0m - \u001b[1mstep 0, loss 0.6189808211788789 in 152.11588406562805 sec\u001b[0m\n",
      "\u001b[32m2025-07-24 15:44:35.420\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mqedft.train.od.train\u001b[0m:\u001b[36mnp_value_and_grad_fn\u001b[0m:\u001b[36m458\u001b[0m - \u001b[1mSave checkpoint /Users/igorsokolov/PycharmProjects/qex/tests/ckpts/ckpt-00000\u001b[0m\n",
      "\u001b[32m2025-07-24 15:44:45.118\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mqedft.train.od.train\u001b[0m:\u001b[36mnp_value_and_grad_fn\u001b[0m:\u001b[36m454\u001b[0m - \u001b[1mstep 1, loss 58.94780483418211 in 0.001081705093383789 sec\u001b[0m\n",
      "\u001b[32m2025-07-24 15:44:54.926\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mqedft.train.od.train\u001b[0m:\u001b[36mnp_value_and_grad_fn\u001b[0m:\u001b[36m454\u001b[0m - \u001b[1mstep 2, loss 0.015827531145158117 in 0.0030786991119384766 sec\u001b[0m\n",
      "\u001b[32m2025-07-24 15:45:05.233\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mqedft.train.od.train\u001b[0m:\u001b[36mnp_value_and_grad_fn\u001b[0m:\u001b[36m454\u001b[0m - \u001b[1mstep 3, loss 0.01579069388139274 in 0.001116037368774414 sec\u001b[0m\n",
      "\u001b[32m2025-07-24 15:45:14.707\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mqedft.train.od.train\u001b[0m:\u001b[36mnp_value_and_grad_fn\u001b[0m:\u001b[36m454\u001b[0m - \u001b[1mstep 4, loss 0.01567442313311589 in 0.0008649826049804688 sec\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import qedft\n",
    "from qedft.models.networks import GlobalQiCQNN, GlobalQiQNN, GlobalQNN\n",
    "from qedft.config.config import Config\n",
    "from qedft.train.od.trainer import KSDFTTrainer\n",
    "from horqrux.utils.operator_utils import DiffMode\n",
    "from loguru import logger\n",
    "\n",
    "# Make sure we use double precision\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "# Get project path\n",
    "project_path = Path(os.path.dirname(os.path.dirname(qedft.__file__)))\n",
    "\n",
    "# Load base configuration\n",
    "config = Config(config_path=project_path / 'qedft' / 'config' / 'train_config.yaml').config\n",
    "\n",
    "# Let's change to settings to have a global functional\n",
    "config['network_type'] = 'conv_dqc'\n",
    "config['use_amplitude_encoding'] = True\n",
    "\n",
    "# Configure GlobalQNN\n",
    "qicnn_config = {\n",
    "    \"network_type\": \"conv_dqc\",\n",
    "    \"wrap_self_interaction\": False,\n",
    "    \"wrap_with_negative_transform\": False,\n",
    "    \"use_amplitude_encoding\": True,\n",
    "    \"n_qubits\": 6,  # Number of qubits in the quantum layer\n",
    "    \"n_var_layers\": 8,  # Number of quantum layers\n",
    "    \"n_layers\": 8,\n",
    "    \"largest_kernel_width\": 4,\n",
    "    \"max_number_conv_layers\": 1,  # Limit to 1 convolutional layer\n",
    "    \"list_qubits_per_layer\": [],\n",
    "    \"force_qubits_per_layer_is_kernel_width\": False,\n",
    "    \"normalization\": 1.0,\n",
    "    \"last_layer_type\": \"dense\",  # Use a linear layer at the end\n",
    "    \"use_bias_mlp\": False,\n",
    "    \"last_layer_features\": [1],\n",
    "    \"diff_mode\": DiffMode.AD,\n",
    "    \"n_shots\": 0,\n",
    "    \"key\": jax.random.PRNGKey(0)\n",
    "}\n",
    "\n",
    "# Initialize the QiCNN network\n",
    "network = GlobalQNN(config_dict=qicnn_config)\n",
    "\n",
    "num_points = 513\n",
    "density = jnp.linspace(0, 1, num_points)\n",
    "init_fn, apply_fn = network.build_network(grids=density)\n",
    "\n",
    "# Initialize parameters and run inference\n",
    "rng_key = qicnn_config['key']\n",
    "_, params = init_fn(rng_key, input_shape=(-1, num_points, 1))\n",
    "output = apply_fn(params, density)\n",
    "\n",
    "# Print the output\n",
    "print(output)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = KSDFTTrainer(\n",
    "    config_dict=config,\n",
    "    network=network,\n",
    "    data_path=project_path / 'data' / 'od'\n",
    ")\n",
    "\n",
    "# Train model\n",
    "params, loss, info = trainer.train(\n",
    "    checkpoint_save_dir=project_path / 'tests' / 'ckpts'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data when we managed to solve the problem of the infinite compilation\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "# 6 qubits 4 layers\n",
    "\n",
    "2025-06-12 14:17:08.406 | INFO     | qedft.train.od.train:_kohn_sham:116 - Jitting kohn_sham_func\n",
    "[6.18994484]\n",
    "list_kernel_dimensions , list_outputs_per_conv_layer:  [3] [171]\n",
    "2025-06-12 14:19:56.677 | INFO     | qedft.train.od.train:np_value_and_grad_fn:454 - step 0, loss 0.6174891506462484 in 156.86434984207153 sec\n",
    "\n",
    "# Took 2 minutes 30 seconds for 6 qubits 4 layers to compile JIT\n",
    "\n",
    "2025-06-12 14:20:47.444 | INFO     | qedft.train.od.train:_kohn_sham:116 - Jitting kohn_sham_func\n",
    "[6.16381374]\n",
    "list_kernel_dimensions , list_outputs_per_conv_layer:  [3] [171]\n",
    "2025-06-12 14:23:51.321 | INFO     | qedft.train.od.train:np_value_and_grad_fn:454 - step 0, loss 0.6201345081753682 in 173.67776083946228 sec\n",
    "\n",
    "# Took 2 minutes 51 seconds for 6 qubits 8 layers to compile JIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example where we have a Bug with GPSR and shots > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-24 15:04:19.523\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mqedft.models.quantum.quantum_models\u001b[0m:\u001b[36mbuild_qnn\u001b[0m:\u001b[36m683\u001b[0m - \u001b[1mBuilding LocalQNN QNN with DirectQNN layer_type\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bitflip rate: 0.2\n",
      "Amplitude damping rate: 0.08\n",
      "Phase damping rate: 0.05\n",
      "Built the network\n",
      "[0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.72 0.72 0.72 0.72\n",
      " 0.72 0.72 0.72 0.72 0.72 0.72 0.72 0.72 0.72 0.72 0.72 0.72 0.72 0.72\n",
      " 0.72 0.72 0.72 0.72 0.72 0.72 0.72 0.72 0.72 0.72 0.72 0.72 0.72 0.72\n",
      " 0.72 0.72 0.72 0.72 0.72 0.72 0.72 0.72 0.72 0.72 0.72 0.72 0.72 0.72\n",
      " 0.72 0.72 0.72 0.72 0.72 0.72 0.72 0.72 0.72 0.68 0.68 0.68 0.68 0.68\n",
      " 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.68\n",
      " 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.68\n",
      " 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.68\n",
      " 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.68\n",
      " 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.68\n",
      " 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.68\n",
      " 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.64 0.64 0.64 0.64 0.64 0.64\n",
      " 0.64 0.64 0.64 0.64 0.64 0.64 0.64 0.64 0.64 0.64 0.64 0.64 0.64 0.64\n",
      " 0.64 0.64 0.64 0.64 0.64 0.64 0.64 0.64 0.64 0.64 0.64 0.64 0.64 0.64\n",
      " 0.64 0.64 0.64 0.64 0.64 0.64 0.62 0.6  0.6  0.6  0.6  0.6  0.6  0.6\n",
      " 0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6\n",
      " 0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6\n",
      " 0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6\n",
      " 0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6\n",
      " 0.6  0.6  0.6  0.6  0.56 0.56 0.56 0.56 0.56 0.56 0.56 0.56 0.56 0.56\n",
      " 0.56 0.56 0.56 0.56 0.56 0.56 0.56 0.56 0.56 0.56 0.56 0.56 0.56 0.56\n",
      " 0.56 0.56 0.56 0.56 0.56 0.56 0.56 0.56 0.56 0.56 0.56 0.56 0.58 0.58\n",
      " 0.58 0.58 0.58 0.58 0.58 0.58 0.58 0.58 0.58 0.58 0.58 0.58 0.58 0.58\n",
      " 0.58 0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6\n",
      " 0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6\n",
      " 0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6\n",
      " 0.6  0.6  0.6  0.6  0.6  0.56 0.56 0.56 0.56 0.56 0.56 0.56 0.56 0.56\n",
      " 0.56 0.56 0.56 0.56 0.56 0.56 0.56 0.56 0.56 0.56 0.56 0.56 0.56 0.56\n",
      " 0.56 0.56 0.56 0.56 0.56 0.56 0.56 0.56 0.56 0.56 0.56 0.56 0.56 0.56\n",
      " 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52\n",
      " 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52\n",
      " 0.52 0.52 0.48 0.48 0.48 0.48 0.48 0.48 0.48 0.48 0.48 0.48 0.48 0.48\n",
      " 0.48 0.48 0.48 0.48 0.48 0.48 0.48 0.48 0.48 0.48 0.48 0.48 0.48 0.48\n",
      " 0.48 0.48 0.48 0.48 0.48 0.48 0.48 0.48 0.48 0.48 0.48 0.48 0.48 0.48\n",
      " 0.48 0.48 0.44 0.44 0.44 0.44 0.44 0.46 0.46 0.46 0.46 0.46 0.46 0.46\n",
      " 0.46 0.46 0.46 0.46 0.46 0.48 0.44 0.44 0.44 0.44 0.44 0.44 0.44 0.44\n",
      " 0.44 0.44 0.44 0.44 0.44 0.44 0.44 0.44 0.46]\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Leaked trace MainTrace(2,BatchTrace). Leaked tracer(s):\n\nTraced<ShapedArray(float64[])>with<BatchTrace(level=2/0)> with\n  val = Traced<ShapedArray(float64[513])>with<DynamicJaxprTrace(level=1/0)>\n  batch_dim = 0\nThis BatchTracer with object id 6048091008 was created on line:\n  /Users/igorsokolov/PycharmProjects/qex/qedft/models/quantum/feature_maps.py:221 (<listcomp>)\n<BatchTracer 6048091008> is referred to by <Parametric 6143206800>.param\n<Parametric 6143206800> is referred to by <list 6142689216>[1]\n<list 6142689216> is referred to by <Unhashable 6143204208>\n<Unhashable 6143204208> is referred to by <tuple 13930302816>[1]\n<tuple 13930302816> is referred to by <tuple 5685653664>[0]\n<tuple 5685653664> is referred to by <tuple 6143323136>[1]\n<tuple 6143323136> is referred to by <tuple 6141947776>[2]\n<tuple 6141947776> is referred to by <WrappedFun 13930313216>\n<WrappedFun 13930313216> is referred to by <function 6139539616> (jvp_jaxpr_thunk) closed-over variable jvp\n<function 6139539616> is referred to by <function 6139541056> (memoized) closed-over variable fn\n<function 6139541056> is referred to by <dict 6141761984>['jvp_jaxpr_thunk']\n<dict 6141761984> is referred to by <JaxprEqn 13887240704>\n<JaxprEqn 13887240704> is referred to by <list 5491941184>[36]\n<list 5491941184> is referred to by <JaxprStackFrame 6042083264>.eqns\n<JaxprStackFrame 6042083264> is referred to by <frame 13911239744>\n<frame 13911239744> is referred to by <generator 5753267232>\n<generator 5753267232> is referred to by <_GeneratorContextManager 5748897008>.gen\n<_GeneratorContextManager 5748897008> is referred to by <method 13948107968>\n\nTraced<ShapedArray(float64[])>with<BatchTrace(level=2/0)> with\n  val = Traced<ShapedArray(float64[513])>with<DynamicJaxprTrace(level=1/0)>\n  batch_dim = 0\nThis BatchTracer with object id 5640434240 was created on line:\n  /Users/igorsokolov/PycharmProjects/qex/qedft/models/quantum/feature_maps.py:221 (<listcomp>)\n<BatchTracer 5640434240> is referred to by <Parametric 6143205888>.param\n<Parametric 6143205888> is referred to by <list 6142689216>[0]\n<list 6142689216> is referred to by <Unhashable 6143204208>\n<Unhashable 6143204208> is referred to by <tuple 13930302816>[1]\n<tuple 13930302816> is referred to by <tuple 5685653664>[0]\n<tuple 5685653664> is referred to by <tuple 6143323136>[1]\n<tuple 6143323136> is referred to by <tuple 6141947776>[2]\n<tuple 6141947776> is referred to by <WrappedFun 13930313216>\n<WrappedFun 13930313216> is referred to by <function 6139539616> (jvp_jaxpr_thunk) closed-over variable jvp\n<function 6139539616> is referred to by <function 6139541056> (memoized) closed-over variable fn\n<function 6139541056> is referred to by <dict 6141761984>['jvp_jaxpr_thunk']\n<dict 6141761984> is referred to by <JaxprEqn 13887240704>\n<JaxprEqn 13887240704> is referred to by <list 5491941184>[36]\n<list 5491941184> is referred to by <JaxprStackFrame 6042083264>.eqns\n<JaxprStackFrame 6042083264> is referred to by <frame 13911239744>\n<frame 13911239744> is referred to by <generator 5753267232>\n<generator 5753267232> is referred to by <_GeneratorContextManager 5748897008>.gen\n<_GeneratorContextManager 5748897008> is referred to by <method 13948107968>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 92\u001b[0m\n\u001b[1;32m     90\u001b[0m rng_key \u001b[38;5;241m=\u001b[39m qicnn_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkey\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     91\u001b[0m _, params \u001b[38;5;241m=\u001b[39m init_fn(rng_key, input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, num_points, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 92\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mjit_apply_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdensity\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Initialize trainer\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "File \u001b[0;32m~/PycharmProjects/qex/qedft/models/quantum/quantum_models.py:774\u001b[0m, in \u001b[0;36mbuild_qnn.<locals>.apply_fn\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    771\u001b[0m qnn \u001b[38;5;241m=\u001b[39m qnn_class()\n\u001b[1;32m    772\u001b[0m \u001b[38;5;66;03m# might cause a bug with wrap_self_interaction=True because of the squeeze() for\u001b[39;00m\n\u001b[1;32m    773\u001b[0m \u001b[38;5;66;03m# global QNN; for local QNN it's fine, tested\u001b[39;00m\n\u001b[0;32m--> 774\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mqnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "File \u001b[0;32m~/PycharmProjects/qex/qedft/models/quantum/quantum_models.py:494\u001b[0m, in \u001b[0;36mLocalQNN.__call__\u001b[0;34m(self, param_values, x)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mndim(x) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    489\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLocalQNN expects inputs of shape (N,) for local processing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    490\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. For inputs with shape (N, P), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    491\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse a global QNN variant instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    492\u001b[0m     )\n\u001b[0;32m--> 494\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 4 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/qadence/lib/python3.10/contextlib.py:142\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 142\u001b[0m         \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/PycharmProjects/qex/.venv_test/lib/python3.10/site-packages/jax/_src/core.py:1245\u001b[0m, in \u001b[0;36mnew_main\u001b[0;34m(trace_type, dynamic, **payload)\u001b[0m\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m t() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1244\u001b[0m   leaked_tracers \u001b[38;5;241m=\u001b[39m maybe_find_leaked_tracers(t())\n\u001b[0;32m-> 1245\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m leaked_tracers: \u001b[38;5;28;01mraise\u001b[39;00m leaked_tracer_error(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrace\u001b[39m\u001b[38;5;124m\"\u001b[39m, t(), leaked_tracers)\n",
      "\u001b[0;31mException\u001b[0m: Leaked trace MainTrace(2,BatchTrace). Leaked tracer(s):\n\nTraced<ShapedArray(float64[])>with<BatchTrace(level=2/0)> with\n  val = Traced<ShapedArray(float64[513])>with<DynamicJaxprTrace(level=1/0)>\n  batch_dim = 0\nThis BatchTracer with object id 6048091008 was created on line:\n  /Users/igorsokolov/PycharmProjects/qex/qedft/models/quantum/feature_maps.py:221 (<listcomp>)\n<BatchTracer 6048091008> is referred to by <Parametric 6143206800>.param\n<Parametric 6143206800> is referred to by <list 6142689216>[1]\n<list 6142689216> is referred to by <Unhashable 6143204208>\n<Unhashable 6143204208> is referred to by <tuple 13930302816>[1]\n<tuple 13930302816> is referred to by <tuple 5685653664>[0]\n<tuple 5685653664> is referred to by <tuple 6143323136>[1]\n<tuple 6143323136> is referred to by <tuple 6141947776>[2]\n<tuple 6141947776> is referred to by <WrappedFun 13930313216>\n<WrappedFun 13930313216> is referred to by <function 6139539616> (jvp_jaxpr_thunk) closed-over variable jvp\n<function 6139539616> is referred to by <function 6139541056> (memoized) closed-over variable fn\n<function 6139541056> is referred to by <dict 6141761984>['jvp_jaxpr_thunk']\n<dict 6141761984> is referred to by <JaxprEqn 13887240704>\n<JaxprEqn 13887240704> is referred to by <list 5491941184>[36]\n<list 5491941184> is referred to by <JaxprStackFrame 6042083264>.eqns\n<JaxprStackFrame 6042083264> is referred to by <frame 13911239744>\n<frame 13911239744> is referred to by <generator 5753267232>\n<generator 5753267232> is referred to by <_GeneratorContextManager 5748897008>.gen\n<_GeneratorContextManager 5748897008> is referred to by <method 13948107968>\n\nTraced<ShapedArray(float64[])>with<BatchTrace(level=2/0)> with\n  val = Traced<ShapedArray(float64[513])>with<DynamicJaxprTrace(level=1/0)>\n  batch_dim = 0\nThis BatchTracer with object id 5640434240 was created on line:\n  /Users/igorsokolov/PycharmProjects/qex/qedft/models/quantum/feature_maps.py:221 (<listcomp>)\n<BatchTracer 5640434240> is referred to by <Parametric 6143205888>.param\n<Parametric 6143205888> is referred to by <list 6142689216>[0]\n<list 6142689216> is referred to by <Unhashable 6143204208>\n<Unhashable 6143204208> is referred to by <tuple 13930302816>[1]\n<tuple 13930302816> is referred to by <tuple 5685653664>[0]\n<tuple 5685653664> is referred to by <tuple 6143323136>[1]\n<tuple 6143323136> is referred to by <tuple 6141947776>[2]\n<tuple 6141947776> is referred to by <WrappedFun 13930313216>\n<WrappedFun 13930313216> is referred to by <function 6139539616> (jvp_jaxpr_thunk) closed-over variable jvp\n<function 6139539616> is referred to by <function 6139541056> (memoized) closed-over variable fn\n<function 6139541056> is referred to by <dict 6141761984>['jvp_jaxpr_thunk']\n<dict 6141761984> is referred to by <JaxprEqn 13887240704>\n<JaxprEqn 13887240704> is referred to by <list 5491941184>[36]\n<list 5491941184> is referred to by <JaxprStackFrame 6042083264>.eqns\n<JaxprStackFrame 6042083264> is referred to by <frame 13911239744>\n<frame 13911239744> is referred to by <generator 5753267232>\n<generator 5753267232> is referred to by <_GeneratorContextManager 5748897008>.gen\n<_GeneratorContextManager 5748897008> is referred to by <method 13948107968>\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# from pathlib import Path\n",
    "# import jax\n",
    "# import jax.numpy as jnp\n",
    "# import qedft\n",
    "# from qedft.models.networks import GlobalQiCQNN, GlobalQiQNN, GlobalQNN\n",
    "# from qedft.config.config import Config\n",
    "# from qedft.train.od.trainer import KSDFTTrainer\n",
    "# from horqrux.utils.operator_utils import DiffMode\n",
    "# from loguru import logger\n",
    "\n",
    "# # Make sure we use double precision\n",
    "# jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "# # Get project path\n",
    "# project_path = Path(os.path.dirname(os.path.dirname(qedft.__file__)))\n",
    "\n",
    "# # Load base configuration\n",
    "# config = Config(config_path=project_path / 'qedft' / 'config' / 'train_config.yaml').config\n",
    "\n",
    "# # Let's change to settings to have a global functional\n",
    "# config['network_type'] = 'dqc'\n",
    "# config['use_amplitude_encoding'] = False\n",
    "\n",
    "# # Configure GlobalQNN\n",
    "# qicnn_config = {\n",
    "#     \"network_type\": \"conv_dqc\",\n",
    "#     \"wrap_self_interaction\": False,\n",
    "#     \"wrap_with_negative_transform\": False,\n",
    "#     \"use_amplitude_encoding\": False,\n",
    "#     \"n_qubits\": 2,  # Number of qubits in the quantum layer\n",
    "#     \"n_var_layers\": 2,  # Number of quantum layers\n",
    "#     \"n_layers\": 2,\n",
    "#     \"largest_kernel_width\": 4,\n",
    "#     \"max_number_conv_layers\": 1,  # Limit to 1 convolutional layer\n",
    "#     \"list_qubits_per_layer\": [],\n",
    "#     \"force_qubits_per_layer_is_kernel_width\": False,\n",
    "#     \"normalization\": 1.0,\n",
    "#     \"last_layer_type\": \"dense\",  # Use a linear layer at the end\n",
    "#     \"use_bias_mlp\": False,\n",
    "#     \"last_layer_features\": [1],\n",
    "#     \"diff_mode\": DiffMode.GPSR,\n",
    "#     \"n_shots\": 100,\n",
    "#     \"key\": jax.random.PRNGKey(0),\n",
    "#     \"qnn_type\": \"LocalQNN\",\n",
    "#     \"layer_type\": \"DirectQNN\"\n",
    "# }\n",
    "# # Initialize the QiCNN network\n",
    "# # network = GlobalQNN(config_dict=qicnn_config)\n",
    "# network = LocalQNN(config_dict=qicnn_config)\n",
    "\n",
    "\n",
    "# # Noise\n",
    "# bitflip_rate = 0.2 # 0.2\n",
    "# amplitude_damping_rate = 0.08 # 0.08\n",
    "# phase_damping_rate = 0.05 # 0.05\n",
    "\n",
    "# noise_scaling = 1. # 1e-6\n",
    "\n",
    "# bitflip_rate *= noise_scaling\n",
    "# amplitude_damping_rate *= noise_scaling\n",
    "# phase_damping_rate *= noise_scaling\n",
    "\n",
    "# # Print the noise rates\n",
    "# print(f\"Bitflip rate: {bitflip_rate}\")\n",
    "# print(f\"Amplitude damping rate: {amplitude_damping_rate}\")\n",
    "# print(f\"Phase damping rate: {phase_damping_rate}\")\n",
    "\n",
    "# noise = (DigitalNoiseInstance(DigitalNoiseType.BITFLIP, bitflip_rate), DigitalNoiseInstance(DigitalNoiseType.AMPLITUDE_DAMPING, amplitude_damping_rate), DigitalNoiseInstance(DigitalNoiseType.PHASE_DAMPING, phase_damping_rate),)\n",
    "\n",
    "# # noise = (DigitalNoiseInstance(DigitalNoiseType.BITFLIP, 0.0), )\n",
    "\n",
    "# num_points = 513\n",
    "# density = jnp.linspace(0, 1, num_points)\n",
    "# init_fn, apply_fn = network.build_network(grids=density, noise=noise)\n",
    "\n",
    "# print(\"Built the network\")\n",
    "\n",
    "# # Initialize parameters and run inference\n",
    "# rng_key = qicnn_config['key']\n",
    "# _, params = init_fn(rng_key, input_shape=(-1, num_points, 1))\n",
    "# output = apply_fn(params, density)\n",
    "# # Print the output\n",
    "# print(output)\n",
    "\n",
    "# # JIT the network\n",
    "# jit_apply_fn = jax.jit(apply_fn)\n",
    "\n",
    "# rng_key = qicnn_config['key']\n",
    "# _, params = init_fn(rng_key, input_shape=(-1, num_points, 1))\n",
    "# output = jit_apply_fn(params, density)\n",
    "# print(output)\n",
    "\n",
    "\n",
    "# # Initialize trainer\n",
    "# trainer = KSDFTTrainer(\n",
    "#     config_dict=config,\n",
    "#     network=network,\n",
    "#     data_path=project_path / 'data' / 'od'\n",
    "# )\n",
    "\n",
    "# # Train model\n",
    "# params, loss, info = trainer.train(\n",
    "#     checkpoint_save_dir=project_path / 'tests' / 'ckpts'\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example where we will use just the Global QNN and noise there\n",
    "- We will add the shot noise as a Gaussian on the output of the QNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
